{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d6137-3120-4f23-9973-a5ec31ab89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd MiniGPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8eae6f-c8b0-4d2e-bafb-c604338de999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import gradio as gr\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.multi_img_conversation import Chat, CONV_VISION\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "import os\n",
    "\n",
    "import argparse as argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b738f-deaa-4f5a-9065-3a163eeda4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper Methods\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--cfg-path\", required=True, help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--num-beams\", type=int, default=2, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--temperature\", type=int, default=0.9, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--english\", type=bool, default=True, help=\"chinese or english\")\n",
    "    parser.add_argument(\"--prompt-en\", type=str, default=\"can you describe the current picture?\", help=\"Can you describe the current picture?\")\n",
    "    parser.add_argument(\"--prompt-zh\", type=str, default=\"你能描述一下当前的图片？\", help=\"Can you describe the current picture?\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "def ask_model(chat_state, image, prompt, img_list):\n",
    "    if image not None:\n",
    "        chat.upload_img(image, chat_state, img_list)\n",
    "    chat.ask(prompt, chat_state)\n",
    "    llm_message = chat.answer(\n",
    "        conv=chat_state,\n",
    "        img_list=img_list,\n",
    "        num_beams=args.num_beams,\n",
    "        temperature=args.temperature,\n",
    "        max_new_tokens=300,\n",
    "        max_length=2000\n",
    "    )[0]\n",
    "    return llm_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a764e0-a5ab-45bc-8aaa-bdae5aa7a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing Chat')\n",
    "#args = parse_args()\n",
    "#args = preset_args\n",
    "args = argparse.Namespace(cfg_path='eval_configs/minigpt4_eval.yaml', gpu_id=0, num_beams=2, temperature=0.9, english=True, prompt_en='can you describe the current picture?', prompt_zh='你能描述一下当前的图片？', options=None)\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))\n",
    "print('Initialization Finished')\n",
    "\n",
    "print('Intializing Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4e457-d7ca-441c-87d7-3ec2c80ce01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigpt4_inference_data = {\"level\" : [],\n",
    "                           \"llm_message\" ; []}\n",
    "\n",
    "prompt1 = \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\n",
    "prompt2 = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\"\"\n",
    "prompt3 = \"\"\"Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\"\"\n",
    "\n",
    "prompts = [prompt1, prompt2, prompt3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c9d803-c3c2-4811-9843-f06a83057e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, prompt in enumerate(prompts):\n",
    "    chat_state = CONV_VISION.copy()\n",
    "    llm_message = ask_model(chat_state, None, prompt, [])\n",
    "    print(llm_message)\n",
    "    minigpt4_inference_data[\"level\"] = num\n",
    "    minigpt4_inference_data[\"llm_message\"] = llm_message\n",
    "    \n",
    "minigpt4_inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144669a2-989e-441b-88ec-4ddde22ccdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "minigpt4_inference_data_df = pd.DataFrame(minigpt4_inference_data_df)\n",
    "minigpt4_inference_data_df.to_csv('minigpt4_inference_data.csv')\n",
    "minigpt4_inference_data_df.head()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
