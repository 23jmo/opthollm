{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'MiniGPT-4'\n",
      "/home/jupyter/opthollm/MiniGPT-4\n"
     ]
    }
   ],
   "source": [
    "%cd MiniGPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Packages\n",
    "Import minigpt4 and necessary helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import gradio as gr\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.multi_img_conversation import Chat, CONV_VISION\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "import os\n",
    "\n",
    "import argparse as argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods\n",
    "Define helper methods including encode diagnosis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Methods\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--cfg-path\", required=True, help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--num-beams\", type=int, default=2, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--temperature\", type=int, default=0.9, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--english\", type=bool, default=True, help=\"chinese or english\")\n",
    "    parser.add_argument(\"--prompt-en\", type=str, default=\"can you describe the current picture?\", help=\"Can you describe the current picture?\")\n",
    "    parser.add_argument(\"--prompt-zh\", type=str, default=\"你能描述一下当前的图片？\", help=\"Can you describe the current picture?\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "\n",
    "### fix this method since it's not completely accurate \n",
    "\n",
    "# determines if the LLM thinks the image is glaucomatous or not based on whether or not the text contains glaucomatous or not \n",
    "def encode_diagnosis(diagnosis):\n",
    "    # could add: if contains glaucomatous and normal, then only look at first sentence\n",
    "    \n",
    "    if 'glaucomatous' in diagnosis.lower():\n",
    "        return 1\n",
    "    if 'normal' in diagnosis.lower():\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# finds the true label of an image based on where it's stored in file path \n",
    "def fetch_ground_truth(img_path):\n",
    "    split_string = img_path.split(\"/\")\n",
    "\n",
    "    # Find the index of \"glaucoma\" in the split string\n",
    "    try:\n",
    "        split_string.index(\"glaucoma\")\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# helper method that gets all files from a directory \n",
    "def get_all_files(directory):\n",
    "    all_files = []\n",
    "    \n",
    "    # Iterate over all the directories and files within the given directory\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            all_files.append(file_path)\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def get_random_file(directory):\n",
    "    all_files = get_all_files(directory)\n",
    "    return random.choice(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat\n",
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d09ad5e13d44719a8a27885d0b943ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load 4 training prompts\n",
      "Prompt Example \n",
      "###Human: <Img><ImageHere></Img> Please provide a detailed description of the picture. ###Assistant: \n",
      "Load BLIP2-LLM Checkpoint: pretrained_minigpt4.pth\n",
      "Initialization Finished\n",
      "Intializing Test\n"
     ]
    }
   ],
   "source": [
    "print('Initializing Chat')\n",
    "#args = parse_args()\n",
    "#args = preset_args\n",
    "args = argparse.Namespace(cfg_path='eval_configs/minigpt4_eval.yaml', gpu_id=0, num_beams=2, temperature=0.9, english=True, prompt_en='can you describe the current picture?', prompt_zh='你能描述一下当前的图片？', options=None)\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))\n",
    "print('Initialization Finished')\n",
    "\n",
    "print('Intializing Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Few Shot Learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store output\n",
    "\n",
    "few_shot_data = {'img_path:': [],\n",
    "                 'prediction:': [],\n",
    "                 'ground_truth:': [],\n",
    "                 'llm_message': []\n",
    "                 }\n",
    "\n",
    "# define examples for few shot learning\n",
    "\n",
    "from chain_of_thought_imgs import img_descriptions\n",
    "examples = img_descriptions.chain_of_thought_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img: RIM-ONE_DL_images/partitioned_randomly/training_set/normal/r2_Im220.png - Prediction: 2 - Ground Truth: 0 - LLM Message: This image shows a cross section of the eye, with the optic nerve highlighted in red. The optic nerve is responsible for transmitting visual information from the retina to the brain. The retina is the innermost layer of the eye, responsible for capturing light and transmitting it to the optic nerve. The macula is the central part of the retina, responsible for sharp, central vision. The fovea is the small, central pit in the macula, responsible for the highest visual acuity.\n",
      "Conversation(system='Give the following image: <Img>ImageContent</Img>. You will be able to see the image once I provide it to you.You are OphthoLLM, an ophthalmology expert AI that diagnosis Glaucoma. You will be given the following fundus image: <Img>ImageContent</Img>. \\n\\nHere are some guidelines for diagnosing Glaucoma using a fundus image. \\n\\nOptic Disc Size: The size of the optic disc should be evaluated, as variations can be a normal characteristic. However, an unusually small or large optic disc may indicate specific conditions or risk factors.\\n\\nCup-to-Disc Ratio: The cup-to-disc ratio measures the size of the cup (the depression in the center of the optic nerve head) relative to the size of the entire optic disc. An increased cup-to-disc ratio may suggest glaucomatous damage.\\n\\nCup Shape: The shape of the cup should be observed, as an asymmetric or vertically elongated cup can be an indication of glaucoma.\\n\\nOptic Disc Rim: The appearance of the neuroretinal rim, which surrounds the cup, is assessed. In glaucoma, this rim tends to thin and become pale or grayish.\\n\\nRim Notching: Notching or notches in the neuroretinal rim, particularly in the inferior and superior regions, can be a characteristic sign of glaucoma.\\n\\nDisc Hemorrhages: Presence of hemorrhages, small bleeding spots, at or around the optic nerve head may indicate glaucomatous damage.\\n\\nNerve Fiber Layer Defects: The doctor will look for thinning or gaps in the retinal nerve fiber layer (RNFL) around the optic nerve head, which is a common early sign of glaucoma.\\n\\nVascular Changes: Changes in the blood vessels, such as vascular narrowing, crossing defects, or bayoneting, may suggest glaucoma or other optic nerve disorders.\\n\\nOptic Disc Color: The color of the optic nerve head is assessed, and any abnormal discoloration, such as pallor or hyperemia, may raise suspicion of optic nerve damage.\\nPeripapillary Atrophy: Doctors will look for areas of atrophy (thinning) of the retinal pigment epithelium around the optic disc, which can be associated with glaucoma.\\n\\nOptic Nerve Head Excavation: The depth of the optic nerve head excavation or the cup depth is evaluated, as increased cupping can indicate glaucomatous damage.\\n\\nPresence of Drusen: In elderly patients, the presence of drusen, small yellowish deposits in the optic disc, should be noted, as they may mimic glaucomatous changes.\\n\\nPlease answer my questions.\\n    ', roles=('Human', 'Assistant'), messages=[['Human', 'Please diagnose the image: '], ['Human', '<Img><ImageHere></Img>'], ['Human', '. Diagnosis: '], ['Assistant', \"Color fundus photography of both eyes in a 71-year old woman with severe open-angle glaucoma. Right eye (left image) demonstrates marked cupping of the optic nerve. Retinal vessels can be seen 'bayoneting' superonasal (arrow). Left eye (right image) demonstrates greater cupping than the right eye with near complete loss of neuroretinal rim and surrounding peripapillary atrophy. Retinal vessels are seen 'bayoneting' inferotemporal with brief loss of visualization upon entering the cup (arrow). Optic nerve heads are magnified in the lower right corner of each image.\"], ['Human', 'Please diagnose the image: '], ['Human', '<Img><ImageHere></Img>'], ['Human', '. Diagnosis: '], ['Assistant', 'The optic nerve shows moderate cupping but there is a prominent inferior notch. The normal sheen of the nerve fiber layer is absent in a distribution radiating temporally from this notch due to cellular loss. The notch and nerve fiber layer defect can be more easily visualized on optical coherence tomography.'], ['Human', 'Please diagnose the image: '], ['Human', '<Img><ImageHere></Img>'], ['Human', '. Diagnosis: '], ['Assistant', 'Normal Fundus. The disk has sharp margins and is normal in color, with a small central cup. Arterioles and venules have normal color, sheen, and course. Background is in normal color. The macula is enclosed by arching temporal vessels. The fovea is located by a central pit.'], ['Human', 'Please diagnose the image: '], ['Human', '<Img><ImageHere></Img>'], ['Human', '. Diagnosis: '], ['Assistant', 'This image shows a cross section of the eye, with the optic nerve highlighted in red. The optic nerve is responsible for transmitting visual information from the retina to the brain. The retina is the innermost layer of the eye, responsible for capturing light and transmitting it to the optic nerve. The macula is the central part of the retina, responsible for sharp, central vision. The fovea is the small, central pit in the macula, responsible for the highest visual acuity.']], offset=2, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, skip_next=False, conv_id=None)\n"
     ]
    }
   ],
   "source": [
    "directory = 'RIM-ONE_DL_images/partitioned_randomly/training_set'\n",
    "# pick random training image to test on\n",
    "\n",
    "img_list = []\n",
    "chat_state = CONV_VISION.copy()\n",
    "\n",
    "image = get_random_file(directory)\n",
    "few_shot_data['img_path:'].append(image)\n",
    "few_shot_data['ground_truth:'].append(fetch_ground_truth(image))\n",
    "\n",
    "\n",
    "\n",
    "# ask the prompt that has multiple examples (few shot inference)\n",
    "\n",
    "#chat.embed_imgs([row[0] for row in examples], img_list)\n",
    "#chat.embed_imgs([image], img_list)\n",
    "\n",
    "chat.few_shot_learning_emb(chat_state, image, examples, img_list)\n",
    "\n",
    "# have the model answer and display \n",
    "llm_message = llm_message = chat.answer(\n",
    "        conv=chat_state,\n",
    "        img_list=img_list,\n",
    "        num_beams=args.num_beams,\n",
    "        temperature=args.temperature,\n",
    "        max_new_tokens=300,\n",
    "        max_length=2000\n",
    "    )[0]\n",
    "\n",
    "few_shot_data['llm_message'].append(llm_message)\n",
    "few_shot_data['prediction:'].append(encode_diagnosis(llm_message))\n",
    "\n",
    "print(f\"Img: {image} - Prediction: {few_shot_data['prediction:'][-1]} - Ground Truth: {few_shot_data['ground_truth:'][-1]} - LLM Message: {few_shot_data['llm_message'][-1]}\")\n",
    "  \n",
    "print(chat_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img: RIM-ONE_DL_images/partitioned_randomly/training_set/normal/r1_Im063.png - Prediction: 2 - Ground Truth: 0 - LLM Message: \n"
     ]
    }
   ],
   "source": [
    "directory = 'RIM-ONE_DL_images/partitioned_randomly/training_set'\n",
    "# pick random training image to test on\n",
    "\n",
    "img_list = []\n",
    "chat_state = CONV_VISION.copy()\n",
    "\n",
    "image = get_random_file(directory)\n",
    "few_shot_data['img_path:'].append(image)\n",
    "few_shot_data['ground_truth:'].append(fetch_ground_truth(image))\n",
    "\n",
    "\n",
    "\n",
    "# ask the prompt that has multiple examples (few shot inference)\n",
    "\n",
    "chat.upload_img(examples[0][0], chat_state, img_list)\n",
    "chat.ask(f\"Please diagnose the image as glaucomatous or normal. Diagnosis: {examples[0][1]}\", conv=chat_state)\n",
    "chat.upload_img(examples[1][0], chat_state, img_list)\n",
    "chat.ask(f\"Please diagnose the image as glaucomatous or normal. Diagnosis: {examples[1][1]}\", conv=chat_state)\n",
    "chat.upload_img(examples[2][0], chat_state, img_list)\n",
    "chat.ask(f\"Please diagnose the image as glaucomatous or normal. Diagnosis: {examples[2][1]}\", conv=chat_state)\n",
    "chat.upload_img(image, chat_state, img_list)\n",
    "chat.ask(f\"Please diagnose the image as glaucomatous or normal. Diagnosis: \", conv=chat_state)\n",
    "\n",
    "# have the model answer and display \n",
    "llm_message = llm_message = chat.answer(\n",
    "        conv=chat_state,\n",
    "        img_list=img_list,\n",
    "        num_beams=args.num_beams,\n",
    "        temperature=args.temperature,\n",
    "        max_new_tokens=300,\n",
    "        max_length=2000\n",
    "    )[0]\n",
    "\n",
    "few_shot_data['llm_message'].append(llm_message)\n",
    "few_shot_data['prediction:'].append(encode_diagnosis(llm_message))\n",
    "\n",
    "print(f\"Img: {image} - Prediction: {few_shot_data['prediction:'][-1]} - Ground Truth: {few_shot_data['ground_truth:'][-1]} - LLM Message: {few_shot_data['llm_message'][-1]}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
